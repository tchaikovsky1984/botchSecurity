{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Dataset\n",
        "\n",
        "```bash\n",
        ".\n",
        "├── Akhil\n",
        "│  ├── audio\n",
        "│  │  ├── Akhil1.mp3\n",
        "│  │  ├── Akhil2.mp3\n",
        "│  │  ├── Akhil3.mp3\n",
        "│  │  ├── Akhil4.mp3\n",
        "│  │  ├── Akhil5.mp3\n",
        "│  │  ├── Akhil6.mp3\n",
        "│  │  ├── Akhil7.mp3\n",
        "│  │  ├── Akhil8.mp3\n",
        "│  │  ├── Akhil9.mp3\n",
        "│  │  └── Akhil10.mp3\n",
        "│  └── images\n",
        "│     ├── Akhil1.jpeg\n",
        "│     ├── Akhil2.jpeg\n",
        "│     ├── Akhil3.jpeg\n",
        "│     ├── Akhil4.jpeg\n",
        "│     ├── Akhil5.jpeg\n",
        "│     └── Akhil6.jpeg\n",
        "├── Alen\n",
        "│  ├── audio\n",
        "│  │  ├── Alen1.mp3\n",
        "│  │  ├── Alen2.mp3\n",
        "│  │  ├── Alen3.mp3\n",
        "│  │  ├── Alen4.mp3\n",
        "│  │  ├── Alen5.mp3\n",
        "│  │  ├── Alen6.mp3\n",
        "│  │  ├── Alen7.mp3\n",
        "│  │  ├── Alen8.mp3\n",
        "│  │  ├── Alen9.mp3\n",
        "│  │  └── Alen10.mp3\n",
        "│  └── images\n",
        "│     ├── Alen1.jpeg\n",
        "│     ├── Alen2.jpeg\n",
        "│     ├── Alen3.jpeg\n",
        "│     ├── Alen4.jpeg\n",
        "│     └── Alen5.jpeg\n",
        "├── Amaan\n",
        "│  ├── audio\n",
        "│  │  ├── Amaan.mp3\n",
        "│  │  ├── Amaan1.mp3\n",
        "│  │  ├── Amaan2.mp3\n",
        "│  │  ├── Amaan3.mp3\n",
        "│  │  ├── Amaan4.mp3\n",
        "│  │  └── Amaan5.mp3\n",
        "│  └── images\n",
        "│     ├── Amaan1.jpeg\n",
        "│     ├── Amaan2.jpeg\n",
        "│     ├── Amaan3.jpeg\n",
        "│     ├── Amaan4.jpeg\n",
        "│     └── Amaan5.jpeg\n",
        "├── Anshul\n",
        "│  ├── audio\n",
        "│  │  ├── Anshul1.mp3\n",
        "│  │  ├── Anshul2.mp3\n",
        "│  │  ├── Anshul3.mp3\n",
        "│  │  ├── Anshul4.mp3\n",
        "│  │  ├── Anshul5.mp3\n",
        "│  │  └── Anshul6.mp3\n",
        "│  └── images\n",
        "│     ├── Anshul1.jpeg\n",
        "│     ├── Anshul2.jpeg\n",
        "│     ├── Anshul3.jpeg\n",
        "│     ├── Anshul4.jpeg\n",
        "│     └── Anshul5.jpeg\n",
        "├── Neha\n",
        "│  ├── audio\n",
        "│  │  ├── Neha1.mp3\n",
        "│  │  ├── Neha2.mp3\n",
        "│  │  ├── Neha3.mp3\n",
        "│  │  ├── Neha4.mp3\n",
        "│  │  ├── Neha5.mp3\n",
        "│  │  ├── Neha6.mp3\n",
        "│  │  ├── Neha7.mp3\n",
        "│  │  ├── Neha8.mp3\n",
        "│  │  ├── Neha9.mp3\n",
        "│  │  └── Neha10.mp3\n",
        "│  └── images\n",
        "│     ├── Neha1.jpeg\n",
        "│     ├── Neha2.jpeg\n",
        "│     ├── Neha3.jpeg\n",
        "│     ├── Neha4.jpeg\n",
        "│     ├── Neha5.jpeg\n",
        "│     └── Neha6.jpeg\n",
        "└── Shirlyn\n",
        "   ├── audio\n",
        "   │  ├── Shirlyn1.mp3\n",
        "   │  ├── Shirlyn2.mp3\n",
        "   │  ├── Shirlyn3.mp3\n",
        "   │  ├── Shirlyn4.mp3\n",
        "   │  ├── Shirlyn5.mp3\n",
        "   │  ├── Shirlyn6.mp3\n",
        "   │  ├── Shirlyn7.mp3\n",
        "   │  ├── Shirlyn8.mp3\n",
        "   │  ├── Shirlyn9.mp3\n",
        "   │  └── Shirlyn10.mp3\n",
        "   └── images\n",
        "      ├── Shirlyn1.jpeg\n",
        "      ├── Shirlyn2.jpeg\n",
        "      ├── Shirlyn3.jpeg\n",
        "      ├── Shirlyn4.jpeg\n",
        "      ├── Shirlyn5.jpeg\n",
        "      └── Shirlyn6.jpeg\n",
        "```\n"
      ],
      "metadata": {
        "id": "ZMl8Kcrw0RPx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NjysZ-kMe186",
        "outputId": "024e422e-5537-4160-c94f-b70ccf5289b3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  dataset.zip\n",
            "   creating: dataset/\n",
            "   creating: dataset/Neha/\n",
            "   creating: dataset/Neha/images/\n",
            "  inflating: dataset/Neha/images/Neha3.jpeg  \n",
            "  inflating: dataset/Neha/images/Neha2.jpeg  \n",
            "  inflating: dataset/Neha/images/Neha6.jpeg  \n",
            "  inflating: dataset/Neha/images/Neha1.jpeg  \n",
            "  inflating: dataset/Neha/images/Neha4.jpeg  \n",
            "  inflating: dataset/Neha/images/Neha5.jpeg  \n",
            "   creating: dataset/Neha/audio/\n",
            "  inflating: dataset/Neha/audio/Neha4.mp3  \n",
            "  inflating: dataset/Neha/audio/Neha8.mp3  \n",
            "  inflating: dataset/Neha/audio/Neha7.mp3  \n",
            "  inflating: dataset/Neha/audio/Neha5.mp3  \n",
            "  inflating: dataset/Neha/audio/Neha6.mp3  \n",
            "  inflating: dataset/Neha/audio/Neha2.mp3  \n",
            "  inflating: dataset/Neha/audio/Neha1.mp3  \n",
            "  inflating: dataset/Neha/audio/Neha10.mp3  \n",
            "  inflating: dataset/Neha/audio/Neha9.mp3  \n",
            "  inflating: dataset/Neha/audio/Neha3.mp3  \n",
            "   creating: dataset/Shirlyn/\n",
            "   creating: dataset/Shirlyn/images/\n",
            "  inflating: dataset/Shirlyn/images/Shirlyn3.jpeg  \n",
            "  inflating: dataset/Shirlyn/images/Shirlyn2.jpeg  \n",
            "  inflating: dataset/Shirlyn/images/Shirlyn1.jpeg  \n",
            "  inflating: dataset/Shirlyn/images/Shirlyn5.jpeg  \n",
            "  inflating: dataset/Shirlyn/images/Shirlyn4.jpeg  \n",
            "  inflating: dataset/Shirlyn/images/Shirlyn6.jpeg  \n",
            "   creating: dataset/Shirlyn/audio/\n",
            "  inflating: dataset/Shirlyn/audio/Shirlyn4.mp3  \n",
            "  inflating: dataset/Shirlyn/audio/Shirlyn2.mp3  \n",
            "  inflating: dataset/Shirlyn/audio/Shirlyn9.mp3  \n",
            "  inflating: dataset/Shirlyn/audio/Shirlyn10.mp3  \n",
            "  inflating: dataset/Shirlyn/audio/Shirlyn7.mp3  \n",
            "  inflating: dataset/Shirlyn/audio/Shirlyn1.mp3  \n",
            "  inflating: dataset/Shirlyn/audio/Shirlyn3.mp3  \n",
            "  inflating: dataset/Shirlyn/audio/Shirlyn5.mp3  \n",
            "  inflating: dataset/Shirlyn/audio/Shirlyn8.mp3  \n",
            "  inflating: dataset/Shirlyn/audio/Shirlyn6.mp3  \n",
            "   creating: dataset/Akhil/\n",
            "   creating: dataset/Akhil/images/\n",
            "  inflating: dataset/Akhil/images/Akhil4.jpeg  \n",
            "  inflating: dataset/Akhil/images/Akhil3.jpeg  \n",
            "  inflating: dataset/Akhil/images/Akhil5.jpeg  \n",
            "  inflating: dataset/Akhil/images/Akhil1.jpeg  \n",
            "  inflating: dataset/Akhil/images/Akhil2.jpeg  \n",
            "  inflating: dataset/Akhil/images/Akhil6.jpeg  \n",
            "   creating: dataset/Akhil/audio/\n",
            "  inflating: dataset/Akhil/audio/Akhil4.mp3  \n",
            "  inflating: dataset/Akhil/audio/Akhil1.mp3  \n",
            "  inflating: dataset/Akhil/audio/Akhil7.mp3  \n",
            "  inflating: dataset/Akhil/audio/Akhil5.mp3  \n",
            "  inflating: dataset/Akhil/audio/Akhil8.mp3  \n",
            "  inflating: dataset/Akhil/audio/Akhil6.mp3  \n",
            "  inflating: dataset/Akhil/audio/Akhil3.mp3  \n",
            "  inflating: dataset/Akhil/audio/Akhil10.mp3  \n",
            "  inflating: dataset/Akhil/audio/Akhil9.mp3  \n",
            "  inflating: dataset/Akhil/audio/Akhil2.mp3  \n",
            "   creating: dataset/Alen/\n",
            "   creating: dataset/Alen/images/\n",
            "  inflating: dataset/Alen/images/Alen4.jpeg  \n",
            "  inflating: dataset/Alen/images/Alen2.jpeg  \n",
            "  inflating: dataset/Alen/images/Alen3.jpeg  \n",
            "  inflating: dataset/Alen/images/Alen1.jpeg  \n",
            "  inflating: dataset/Alen/images/Alen5.jpeg  \n",
            "   creating: dataset/Alen/audio/\n",
            "  inflating: dataset/Alen/audio/Alen10.mp3  \n",
            "  inflating: dataset/Alen/audio/Alen9.mp3  \n",
            "  inflating: dataset/Alen/audio/Alen6.mp3  \n",
            "  inflating: dataset/Alen/audio/Alen3.mp3  \n",
            "  inflating: dataset/Alen/audio/Alen7.mp3  \n",
            "  inflating: dataset/Alen/audio/Alen8.mp3  \n",
            "  inflating: dataset/Alen/audio/Alen5.mp3  \n",
            "  inflating: dataset/Alen/audio/Alen4.mp3  \n",
            "  inflating: dataset/Alen/audio/Alen1.mp3  \n",
            "  inflating: dataset/Alen/audio/Alen2.mp3  \n",
            "   creating: dataset/Anshul/\n",
            "   creating: dataset/Anshul/images/\n",
            "  inflating: dataset/Anshul/images/Anshul5.jpeg  \n",
            "  inflating: dataset/Anshul/images/Anshul2.jpeg  \n",
            "  inflating: dataset/Anshul/images/Anshul3.jpeg  \n",
            "  inflating: dataset/Anshul/images/Anshul1.jpeg  \n",
            "  inflating: dataset/Anshul/images/Anshul4.jpeg  \n",
            "   creating: dataset/Anshul/audio/\n",
            "  inflating: dataset/Anshul/audio/Anshul2.mp3  \n",
            "  inflating: dataset/Anshul/audio/Anshul3.mp3  \n",
            "  inflating: dataset/Anshul/audio/Anshul5.mp3  \n",
            "  inflating: dataset/Anshul/audio/Anshul6.mp3  \n",
            "  inflating: dataset/Anshul/audio/Anshul1.mp3  \n",
            "  inflating: dataset/Anshul/audio/Anshul4.mp3  \n",
            "   creating: dataset/Amaan/\n",
            "   creating: dataset/Amaan/images/\n",
            "  inflating: dataset/Amaan/images/Amaan1.jpeg  \n",
            "  inflating: dataset/Amaan/images/Amaan5.jpeg  \n",
            "  inflating: dataset/Amaan/images/Amaan3.jpeg  \n",
            "  inflating: dataset/Amaan/images/Amaan4.jpeg  \n",
            "  inflating: dataset/Amaan/images/Amaan2.jpeg  \n",
            "   creating: dataset/Amaan/audio/\n",
            "  inflating: dataset/Amaan/audio/Amaan3.mp3  \n",
            "  inflating: dataset/Amaan/audio/Amaan2.mp3  \n",
            "  inflating: dataset/Amaan/audio/Amaan1.mp3  \n",
            "  inflating: dataset/Amaan/audio/Amaan4.mp3  \n",
            "  inflating: dataset/Amaan/audio/Amaan5.mp3  \n",
            "  inflating: dataset/Amaan/audio/Amaan.mp3  \n"
          ]
        }
      ],
      "source": [
        "!unzip dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports"
      ],
      "metadata": {
        "id": "wiNXTLi64fx5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torchvision import models\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "from torchvision import transforms\n",
        "from PIL import Image\n",
        "import torchaudio\n",
        "import torchaudio.transforms as T\n",
        "import numpy as np\n",
        "from collections import defaultdict\n",
        "import random\n",
        "import glob\n",
        "import numpy as np\n",
        "import time"
      ],
      "metadata": {
        "id": "_HuB9Ams0gnW"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class MultiModalPersonDataset(Dataset):\n",
        "    def __init__(self, root_dir, audio_sample_rate=16000, audio_n_mfcc=40, audio_max_len=500, is_train=True):\n",
        "        self.root_dir = root_dir\n",
        "        self.audio_sample_rate = audio_sample_rate\n",
        "        self.audio_n_mfcc = audio_n_mfcc\n",
        "        self.audio_max_len = audio_max_len\n",
        "        self.is_train = is_train\n",
        "\n",
        "        self.persons = sorted([d for d in os.listdir(root_dir) if os.path.isdir(os.path.join(root_dir, d))])\n",
        "        self.person_to_idx = {person: idx for idx, person in enumerate(self.persons)}\n",
        "        self.idx_to_person = {idx: person for person, idx in self.person_to_idx.items()}\n",
        "\n",
        "        # Add the new 'not identified' class\n",
        "        self.not_identified_label = len(self.persons) # Store the index of 'not identified'\n",
        "        self.person_to_idx['not identified'] = self.not_identified_label\n",
        "        self.idx_to_person[self.not_identified_label] = 'not identified'\n",
        "        self.num_classes = len(self.person_to_idx) # Update num_classes to include 'not identified'\n",
        "\n",
        "        self.all_image_paths = defaultdict(list)\n",
        "        self.all_audio_paths = defaultdict(list)\n",
        "        self._collect_paths()\n",
        "\n",
        "        # This method is crucial for balancing your dataset\n",
        "        self.data_pairs = self._create_data_pairs_with_mismatched()\n",
        "\n",
        "        if self.is_train:\n",
        "            self.transform_image = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.RandomHorizontalFlip(), # Good for data augmentation\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "        else: # Validation/Test\n",
        "            self.transform_image = transforms.Compose([\n",
        "                transforms.Resize((224, 224)),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ])\n",
        "\n",
        "        self.mfcc_transform = T.MFCC(\n",
        "            sample_rate=self.audio_sample_rate,\n",
        "            n_mfcc=self.audio_n_mfcc,\n",
        "            melkwargs={\"n_fft\": 400, \"hop_length\": 160, \"n_mels\": 64}\n",
        "        )\n",
        "\n",
        "    def _collect_paths(self):\n",
        "        for person_name in self.persons: # Iterate only through original persons\n",
        "            image_dir = os.path.join(self.root_dir, person_name, 'images')\n",
        "            audio_dir = os.path.join(self.root_dir, person_name, 'audio')\n",
        "\n",
        "            # Collect image paths\n",
        "            if os.path.exists(image_dir):\n",
        "                 for f in os.listdir(image_dir):\n",
        "                    if f.lower().endswith(('.jpeg', '.jpg', '.png')):\n",
        "                        self.all_image_paths[person_name].append(os.path.join(image_dir, f))\n",
        "            else:\n",
        "                print(f\"Warning: Image directory not found for {person_name}: {image_dir}\")\n",
        "\n",
        "            # Collect audio paths\n",
        "            if os.path.exists(audio_dir):\n",
        "                for f in os.listdir(audio_dir):\n",
        "                    if f.lower().endswith(('.mp3', '.m4a', '.aac')):\n",
        "                        self.all_audio_paths[person_name].append(os.path.join(audio_dir, f))\n",
        "            else:\n",
        "                 print(f\"Warning: Audio directory not found for {person_name}: {audio_dir}\")\n",
        "\n",
        "\n",
        "    def _create_data_pairs_with_mismatched(self):\n",
        "        pairs = []\n",
        "        original_person_names = sorted(self.persons)\n",
        "\n",
        "        # 1. Create all matched pairs\n",
        "        matched_pairs = []\n",
        "        for person_name in original_person_names:\n",
        "            image_paths = self.all_image_paths[person_name]\n",
        "            audio_paths = self.all_audio_paths[person_name]\n",
        "            label = self.person_to_idx[person_name]\n",
        "\n",
        "            for img_path in image_paths:\n",
        "                for aud_path in audio_paths:\n",
        "                    matched_pairs.append((img_path, aud_path, label))\n",
        "\n",
        "        pairs.extend(matched_pairs) # Add all matched pairs to the main list\n",
        "\n",
        "        # 2. Create mismatched pairs by sampling\n",
        "        mismatched_pairs = []\n",
        "        # Aim for a 1:1 ratio. Adjust if you want more (e.g., 2 * len(matched_pairs))\n",
        "        target_mismatched_count = len(matched_pairs)\n",
        "\n",
        "        # Get flattened lists of all images and all audios with their true person\n",
        "        all_images_flat = []\n",
        "        for p_name, paths in self.all_image_paths.items():\n",
        "            for path in paths:\n",
        "                all_images_flat.append((path, p_name))\n",
        "\n",
        "        all_audios_flat = []\n",
        "        for p_name, paths in self.all_audio_paths.items():\n",
        "            for path in paths:\n",
        "                all_audios_flat.append((path, p_name))\n",
        "\n",
        "        # Use a set to store (img_path, aud_path) tuples to avoid adding duplicate mismatched pairs\n",
        "        mismatched_set = set()\n",
        "\n",
        "        # Loop until we reach the target count for mismatched pairs\n",
        "        # Add a safety break to prevent infinite loops if data is insufficient for target_mismatched_count\n",
        "        iteration_limit = target_mismatched_count * 5 # Allow more iterations than samples for safety\n",
        "        current_iterations = 0\n",
        "\n",
        "        while len(mismatched_set) < target_mismatched_count and current_iterations < iteration_limit:\n",
        "            current_iterations += 1\n",
        "            if not all_images_flat or not all_audios_flat: # Safety break if no image/audio data at all\n",
        "                print(\"Warning: Not enough image/audio data to create mismatched pairs.\")\n",
        "                break\n",
        "\n",
        "            # Randomly pick an image and an audio\n",
        "            img_data = random.choice(all_images_flat)\n",
        "            aud_data = random.choice(all_audios_flat)\n",
        "\n",
        "            img_path, img_person = img_data\n",
        "            aud_path, aud_person = aud_data\n",
        "\n",
        "            # Check if they are from different persons AND if this specific pair hasn't been added yet\n",
        "            if img_person != aud_person:\n",
        "                if (img_path, aud_path) not in mismatched_set:\n",
        "                    mismatched_set.add((img_path, aud_path))\n",
        "                    mismatched_pairs.append((img_path, aud_path, self.not_identified_label))\n",
        "\n",
        "        pairs.extend(mismatched_pairs) # Add the sampled mismatched pairs\n",
        "\n",
        "        # Shuffle the combined list of pairs to thoroughly mix matched and mismatched data\n",
        "        random.shuffle(pairs)\n",
        "\n",
        "        print(f\"Dataset creation completed. Matched pairs: {len(matched_pairs)}, Mismatched pairs sampled: {len(mismatched_pairs)}\")\n",
        "        return pairs\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data_pairs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        img_path, aud_path, label = self.data_pairs[idx]\n",
        "\n",
        "        image = Image.open(img_path).convert('RGB')\n",
        "        image = self.transform_image(image)\n",
        "\n",
        "        try:\n",
        "            waveform, sample_rate = torchaudio.load(aud_path)\n",
        "            if sample_rate != self.audio_sample_rate:\n",
        "                resampler = T.Resample(sample_rate, self.audio_sample_rate)\n",
        "                waveform = resampler(waveform)\n",
        "\n",
        "            if waveform.shape[0] > 1:\n",
        "                waveform = torch.mean(waveform, dim=0, keepdim=True)\n",
        "\n",
        "            mfcc_features = self.mfcc_transform(waveform).squeeze(0)\n",
        "            mfcc_features = mfcc_features.transpose(0, 1)\n",
        "\n",
        "            if mfcc_features.shape[0] > self.audio_max_len:\n",
        "                mfcc_features = mfcc_features[:self.audio_max_len, :]\n",
        "            elif mfcc_features.shape[0] < self.audio_max_len:\n",
        "                padding = torch.zeros(self.audio_max_len - mfcc_features.shape[0], self.audio_n_mfcc, dtype=mfcc_features.dtype)\n",
        "                mfcc_features = torch.cat((mfcc_features, padding), dim=0)\n",
        "\n",
        "            mean = mfcc_features.mean()\n",
        "            std = mfcc_features.std()\n",
        "            if std == 0: std = 1e-6\n",
        "            mfcc_features = (mfcc_features - mean) / std\n",
        "\n",
        "        except Exception as e:\n",
        "            print(f\"Error loading audio file {aud_path}: {e}\")\n",
        "            mfcc_features = torch.zeros(self.audio_max_len, self.audio_n_mfcc)\n",
        "\n",
        "        return image, mfcc_features, label"
      ],
      "metadata": {
        "id": "iyL-zIXlLhwr"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MultiModalPersonDataset(root_dir='dataset')\n",
        "print(f\"Number of rows in the dataset: {len(dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9-zTW9FvgbbZ",
        "outputId": "c8c896d8-b8f3-4334-f306-a1e82978cc07"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset creation completed. Matched pairs: 290, Mismatched pairs sampled: 290\n",
            "Number of rows in the dataset: 580\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "label_counts = defaultdict(int)\n",
        "for _, _, label in dataset.data_pairs:\n",
        "    label_counts[dataset.idx_to_person[label]] += 1\n",
        "\n",
        "print(\"Label distribution:\")\n",
        "for label_name, count in sorted(label_counts.items()):\n",
        "    print(f\"{label_name}: {count}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SmpGe0fdg56H",
        "outputId": "3a072085-13ea-44c5-8ed1-17b5b46f2be8"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Label distribution:\n",
            "Akhil: 60\n",
            "Alen: 50\n",
            "Amaan: 30\n",
            "Anshul: 30\n",
            "Neha: 60\n",
            "Shirlyn: 60\n",
            "not identified: 290\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Display some matched and mismatched rows\n",
        "print(\"\\nExample Matched and Mismatched Pairs:\")\n",
        "matched_examples = []\n",
        "mismatched_examples = []\n",
        "\n",
        "# Iterate through the data_pairs and collect examples\n",
        "for img_path, aud_path, label in dataset.data_pairs:\n",
        "    label_name = dataset.idx_to_person[label]\n",
        "    pair = (img_path, aud_path, label_name)\n",
        "\n",
        "    # Check if the image and audio paths belong to the same person based on their filenames\n",
        "    img_person = os.path.basename(os.path.dirname(os.path.dirname(img_path)))\n",
        "    aud_person = os.path.basename(os.path.dirname(os.path.dirname(aud_path)))\n",
        "\n",
        "    if img_person == aud_person:\n",
        "        if len(matched_examples) < 5: # Show up to 5 matched examples\n",
        "            matched_examples.append(pair)\n",
        "    else:\n",
        "        if len(mismatched_examples) < 5: # Show up to 5 mismatched examples\n",
        "            mismatched_examples.append(pair)\n",
        "\n",
        "    # Break if we have enough examples of both\n",
        "    if len(matched_examples) >= 5 and len(mismatched_examples) >= 5:\n",
        "        break\n",
        "\n",
        "\n",
        "print(\"\\n--- Matched Examples ---\")\n",
        "if matched_examples:\n",
        "    for img_path, aud_path, label_name in matched_examples:\n",
        "        print(f\"Image: {os.path.basename(img_path)}, Audio: {os.path.basename(aud_path)}, Predicted Label: {label_name}\")\n",
        "else:\n",
        "    print(\"No matched examples found.\")\n",
        "\n",
        "print(\"\\n--- Mismatched Examples ---\")\n",
        "if mismatched_examples:\n",
        "    for img_path, aud_path, label_name in mismatched_examples:\n",
        "         # Determine the *true* persons for image and audio for clarity\n",
        "        img_person = os.path.basename(os.path.dirname(os.path.dirname(img_path)))\n",
        "        aud_person = os.path.basename(os.path.dirname(os.path.dirname(aud_path)))\n",
        "        print(f\"Image: {os.path.basename(img_path)} (True Person: {img_person}), Audio: {os.path.basename(aud_path)} (True Person: {aud_person}), Predicted Label: {label_name}\")\n",
        "else:\n",
        "    print(\"No mismatched examples found.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TZQGDqK-hFgR",
        "outputId": "16caf64f-fc13-4e01-c9a8-5e80b94786a3"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Example Matched and Mismatched Pairs:\n",
            "\n",
            "--- Matched Examples ---\n",
            "Image: Shirlyn2.jpeg, Audio: Shirlyn4.mp3, Predicted Label: Shirlyn\n",
            "Image: Neha6.jpeg, Audio: Neha1.mp3, Predicted Label: Neha\n",
            "Image: Neha2.jpeg, Audio: Neha10.mp3, Predicted Label: Neha\n",
            "Image: Anshul1.jpeg, Audio: Anshul3.mp3, Predicted Label: Anshul\n",
            "Image: Akhil1.jpeg, Audio: Akhil9.mp3, Predicted Label: Akhil\n",
            "\n",
            "--- Mismatched Examples ---\n",
            "Image: Amaan4.jpeg (True Person: Amaan), Audio: Shirlyn10.mp3 (True Person: Shirlyn), Predicted Label: not identified\n",
            "Image: Neha5.jpeg (True Person: Neha), Audio: Anshul2.mp3 (True Person: Anshul), Predicted Label: not identified\n",
            "Image: Akhil6.jpeg (True Person: Akhil), Audio: Shirlyn1.mp3 (True Person: Shirlyn), Predicted Label: not identified\n",
            "Image: Shirlyn5.jpeg (True Person: Shirlyn), Audio: Alen8.mp3 (True Person: Alen), Predicted Label: not identified\n",
            "Image: Shirlyn6.jpeg (True Person: Shirlyn), Audio: Akhil2.mp3 (True Person: Akhil), Predicted Label: not identified\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the split ratios\n",
        "train_ratio = 0.8\n",
        "test_ratio = 0.2\n",
        "\n",
        "# Calculate the number of samples for each split\n",
        "total_size = len(dataset)\n",
        "train_size = int(train_ratio * total_size)\n",
        "test_size = total_size - train_size # Ensure the sum is equal to the total size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "print(f\"\\nTotal dataset size: {total_size}\")\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sY_uqtfksfRk",
        "outputId": "eadf07c6-1b3a-49ee-c7b2-2874d3437c91"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total dataset size: 580\n",
            "Training dataset size: 464\n",
            "Test dataset size: 116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(train_dataset.dataset.data_pairs[0])\n",
        "print(train_dataset)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WlEAigMPsupZ",
        "outputId": "61fdfc1a-d12f-4011-d3c4-4eed74b663f8"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "('dataset/Amaan/images/Amaan4.jpeg', 'dataset/Shirlyn/audio/Shirlyn10.mp3', 6)\n",
            "<torch.utils.data.dataset.Subset object at 0x793f4d17ca10>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class ImageCNN_Complex(nn.Module): # Renamed the class to indicate complexity\n",
        "    def __init__(self, embedding_dim=512):\n",
        "        super(ImageCNN_Complex, self).__init__()\n",
        "        # Use ResNet50 as the backbone\n",
        "        self.backbone = models.resnet50(weights=None) # Using ResNet50 with no pre-trained weights initially\n",
        "\n",
        "        # Freeze backbone parameters\n",
        "        for param in self.backbone.parameters():\n",
        "            param.requires_grad = False\n",
        "\n",
        "        # Replace the classifier to output our desired embedding_dim\n",
        "        # ResNet50's classifier is self.backbone.fc\n",
        "        num_ftrs = self.backbone.fc.in_features\n",
        "        self.backbone.fc = nn.Linear(num_ftrs, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.backbone(x)"
      ],
      "metadata": {
        "id": "loocYpA3J_im"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AudioRNN_Complex(nn.Module): # Renamed the class to indicate complexity\n",
        "    def __init__(self, input_dim, hidden_dim=256, num_layers=2, embedding_dim=512): # Increased defaults\n",
        "        super(AudioRNN_Complex, self).__init__()\n",
        "        # Use GRU\n",
        "        self.gru = nn.GRU(input_dim, hidden_dim, num_layers, batch_first=True, bidirectional=True)\n",
        "        # Output of bidirectional GRU is hidden_dim * 2\n",
        "        self.fc = nn.Linear(hidden_dim * 2, embedding_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        gru_out, hidden = self.gru(x)\n",
        "        # For GRU, hidden shape is (num_layers * num_directions, batch, hidden_size)\n",
        "        # We take the last layer's hidden state for both directions\n",
        "        hidden = torch.cat((hidden[-2,:,:], hidden[-1,:,:]), dim=1)\n",
        "        embedding = self.fc(hidden)\n",
        "        return embedding"
      ],
      "metadata": {
        "id": "ETxPajCrKNDK"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class AttentionFusionModel(nn.Module): # Renamed for clarity\n",
        "    def __init__(self, num_classes, image_embedding_dim=512, audio_embedding_dim=512, attention_dim=128):\n",
        "        super(AttentionFusionModel, self).__init__()\n",
        "        # Ensure train_dataset is defined before running this cell to access its attributes\n",
        "        self.image_model = ImageCNN_Complex(embedding_dim=image_embedding_dim)\n",
        "        # Access audio_n_mfcc from the original dataset object via .dataset\n",
        "        self.audio_model = AudioRNN_Complex(input_dim=train_dataset.dataset.audio_n_mfcc, embedding_dim=audio_embedding_dim)\n",
        "\n",
        "        self.image_embedding_dim = image_embedding_dim\n",
        "        self.audio_embedding_dim = audio_embedding_dim\n",
        "        self.attention_dim = attention_dim # Store attention_dim as an instance variable\n",
        "\n",
        "        # Attention Mechanism layers\n",
        "        self.attention_layer1 = nn.Linear(self.image_embedding_dim + self.audio_embedding_dim, self.attention_dim)\n",
        "        self.attention_layer2 = nn.Linear(self.attention_dim, 2) # Output 2 scores: one for image, one for audio\n",
        "\n",
        "        # Linear layer for final classification\n",
        "        self.fusion_fc = nn.Linear(self.image_embedding_dim + self.audio_embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, image_input, audio_input):\n",
        "        # Get embeddings from individual models\n",
        "        image_embedding = self.image_model(image_input)\n",
        "        audio_embedding = self.audio_model(audio_input)\n",
        "\n",
        "        # Concatenate original embeddings for attention\n",
        "        combined_original = torch.cat((image_embedding, audio_embedding), dim=1)\n",
        "\n",
        "        # Calculate attention weights\n",
        "        attention_scores = self.attention_layer2(torch.tanh(self.attention_layer1(combined_original))) # Shape: (batch_size, 2)\n",
        "        attention_weights = F.softmax(attention_scores, dim=1) # Softmax across the 2 modalities\n",
        "\n",
        "        # Split attention weights\n",
        "        # Access attention_dim using self.attention_dim\n",
        "        image_attn_weight = attention_weights[:, 0].unsqueeze(1) # Shape: (batch_size, 1)\n",
        "        audio_attn_weight = attention_weights[:, 1].unsqueeze(1) # Shape: (batch_size, 1)\n",
        "\n",
        "        # Apply weights to original embeddings\n",
        "        # Need to unsqueeze weights to match embedding dimensions for multiplication\n",
        "        image_attended = image_embedding * image_attn_weight.expand_as(image_embedding)\n",
        "        audio_attended = audio_embedding * audio_attn_weight.expand_as(audio_embedding)\n",
        "\n",
        "        # Concatenate attended embeddings\n",
        "        combined_attended_embedding = torch.cat((image_attended, audio_attended), dim=1)\n",
        "\n",
        "        # Pass through the final classification layer\n",
        "        output = self.fusion_fc(combined_attended_embedding)\n",
        "\n",
        "        return output\n",
        "\n",
        "# Example Usage (update model name)\n",
        "# Ensure train_dataset is defined before running this cell\n",
        "# Access num_classes from the original dataset object via .dataset\n",
        "num_classes = train_dataset.dataset.num_classes\n",
        "model = AttentionFusionModel(num_classes=num_classes) # Changed model name\n",
        "print(model)\n",
        "\n",
        "# Dummy forward pass to check output shape\n",
        "dummy_images = torch.randn(16, 3, 224, 224) # Batch size 16, 3 channels, 224x224\n",
        "# Ensure train_dataset is defined before running this cell to access its attributes\n",
        "# Access audio_n_mfcc from the original dataset object via .dataset\n",
        "max_audio_frames = train_dataset.dataset.audio_max_len\n",
        "dummy_audios = torch.randn(16, max_audio_frames, train_dataset.dataset.audio_n_mfcc) # Batch size 16, max_len, n_mfcc\n",
        "\n",
        "with torch.no_grad(): # No need to compute gradients for this test\n",
        "    output = model(dummy_images, dummy_audios)\n",
        "    print(f\"Output shape: {output.shape}\") # Expected: (batch_size, num_classes)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qZ5uVG2sLK4c",
        "outputId": "4b0f2739-d01c-444d-b086-94f56a383f19"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AttentionFusionModel(\n",
            "  (image_model): ImageCNN_Complex(\n",
            "    (backbone): ResNet(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "      (fc): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (audio_model): AudioRNN_Complex(\n",
            "    (gru): GRU(40, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            "  (attention_layer1): Linear(in_features=1024, out_features=128, bias=True)\n",
            "  (attention_layer2): Linear(in_features=128, out_features=2, bias=True)\n",
            "  (fusion_fc): Linear(in_features=1024, out_features=7, bias=True)\n",
            ")\n",
            "Output shape: torch.Size([16, 7])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "dataset = MultiModalPersonDataset(root_dir='dataset')\n",
        "print(f\"Number of rows in the dataset: {len(dataset)}\")\n",
        "\n",
        "label_counts = defaultdict(int)\n",
        "for _, _, label in dataset.data_pairs:\n",
        "    label_counts[dataset.idx_to_person[label]] += 1\n",
        "\n",
        "print(\"Label distribution:\")\n",
        "for label_name, count in sorted(label_counts.items()):\n",
        "    print(f\"{label_name}: {count}\")\n",
        "\n",
        "# Define the split ratios\n",
        "train_ratio = 0.8\n",
        "test_ratio = 0.2\n",
        "\n",
        "# Calculate the number of samples for each split\n",
        "total_size = len(dataset)\n",
        "train_size = int(train_ratio * total_size)\n",
        "test_size = total_size - train_size # Ensure the sum is equal to the total size\n",
        "\n",
        "# Split the dataset\n",
        "train_dataset, test_dataset = random_split(dataset, [train_size, test_size])\n",
        "\n",
        "print(f\"\\nTotal dataset size: {total_size}\")\n",
        "print(f\"Training dataset size: {len(train_dataset)}\")\n",
        "print(f\"Test dataset size: {len(test_dataset)}\")\n",
        "\n",
        "# Device configuration\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f'Using device: {device}')\n",
        "\n",
        "BATCH_SIZE = 32\n",
        "NUM_WORKERS = 4 # Adjust based on your system's CPU cores and RAM\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, num_workers=NUM_WORKERS)\n",
        "test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS)\n",
        "\n",
        "print(f\"\\nTrain DataLoader created with {len(train_loader)} batches.\")\n",
        "print(f\"Test DataLoader created with {len(test_loader)} batches.\")\n",
        "\n",
        "# Get num_classes from the dataset object\n",
        "num_classes = dataset.num_classes\n",
        "\n",
        "# Hyperparameters\n",
        "num_epochs = 10 # Increased epochs - Consider increasing further if needed\n",
        "learning_rate = 0.001 # Kept learning rate the same\n",
        "weight_decay = 1e-4 # Added L2 regularization (weight decay)\n",
        "# batch_size is set in the DataLoader\n",
        "\n",
        "# --- CRITICAL: Initialize AttentionFusionModel's audio_model ---\n",
        "# The input_dim for AudioRNN_Complex comes from the dataset's audio_n_mfcc\n",
        "model = AttentionFusionModel(num_classes=num_classes).to(device)\n",
        "model.audio_model = AudioRNN_Complex(input_dim=dataset.audio_n_mfcc, embedding_dim=model.audio_embedding_dim).to(device)\n",
        "print(model) # Print model architecture\n",
        "\n",
        "# --- ADD CLASS WEIGHTS CALCULATION HERE ---\n",
        "print(\"\\nCalculating class weights for CrossEntropyLoss...\")\n",
        "\n",
        "# 1. Get the class counts from your training data split\n",
        "# We use 'dataset.data_pairs[i][2]' because 'train_dataset' is a Subset view\n",
        "# on the original 'dataset', and its 'indices' point back to 'dataset'.\n",
        "train_labels_list = [dataset.data_pairs[i][2] for i in train_dataset.indices]\n",
        "\n",
        "class_counts = defaultdict(int)\n",
        "for label_idx in train_labels_list:\n",
        "    class_counts[label_idx] += 1\n",
        "\n",
        "print(f\"Raw class counts in training data split: {class_counts}\")\n",
        "\n",
        "# 2. Calculate the weights\n",
        "total_samples_in_train_split = len(train_labels_list)\n",
        "# num_classes is already derived from 'dataset.num_classes' above\n",
        "\n",
        "class_weights = torch.zeros(num_classes)\n",
        "for i in range(num_classes):\n",
        "    count = class_counts[i]\n",
        "    if count == 0:\n",
        "        class_weights[i] = 0.0 # Assign 0 if a class is completely absent in the split\n",
        "        print(f\"Warning: Class {dataset.idx_to_person[i]} (ID {i}) has 0 samples in training data split.\")\n",
        "    elif i == dataset.not_identified_label:\n",
        "        # Assign base inverse frequency, then multiply by the boost factor\n",
        "        base_weight = total_samples_in_train_split / (num_classes * count)\n",
        "        class_weights[i] = base_weight * 3.97\n",
        "    elif i == dataset.person_to_idx[\"Alen\"] or i == dataset.person_to_idx[\"Shirlyn\"]:\n",
        "        class_weights[i] = total_samples_in_train_split / (num_classes * count) * 1.3\n",
        "    else:\n",
        "        # Inverse frequency weighting: total_samples / (num_classes * count_of_class)\n",
        "        class_weights[i] = total_samples_in_train_split / (num_classes * count)\n",
        "\n",
        "print(f\"Calculated class weights: {class_weights}\")\n",
        "\n",
        "# 3. Move weights to the same device as your model\n",
        "class_weights = class_weights.to(device)\n",
        "\n",
        "# --- LOSS AND OPTIMIZER (CRITICAL: Single criterion as your model has one output) ---\n",
        "# Your AttentionFusionModel returns a single output, so we need only one criterion\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights) # Pass the calculated weights here\n",
        "optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "# Optional: Learning Rate Scheduler (Example: Reduce LR on plateau)\n",
        "# from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
        "# scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=5, verbose=True)\n",
        "\n",
        "\n",
        "# --- Training loop ---\n",
        "print(\"\\nStarting training...\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    correct_train = 0\n",
        "    total_train = 0\n",
        "\n",
        "    start_time = time.time() # Start time for the epoch\n",
        "\n",
        "    for i, (images, audios, labels) in enumerate(train_loader):\n",
        "        # Move tensors to the configured device\n",
        "        images = images.to(device)\n",
        "        audios = audios.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        # Forward pass: Your AttentionFusionModel returns a single 'output'\n",
        "        outputs = model(images, audios)\n",
        "\n",
        "        # Calculate loss using the single, weighted criterion\n",
        "        loss = criterion(outputs, labels)\n",
        "\n",
        "        # Backward and optimize\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "\n",
        "        # Calculate accuracy based on the single output\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        total_train += labels.size(0)\n",
        "        correct_train += (predicted == labels).sum().item()\n",
        "\n",
        "        if (i+1) % 10 == 0: # Print progress every 10 batches\n",
        "            print (f'Epoch [{epoch+1}/{num_epochs}], Step [{i+1}/{len(train_loader)}], Loss: {loss.item():.4f}')\n",
        "\n",
        "    epoch_time = time.time() - start_time\n",
        "    epoch_loss = running_loss / len(train_loader)\n",
        "    epoch_acc = 100 * correct_train / total_train\n",
        "\n",
        "    print(f'Epoch [{epoch+1}/{num_epochs}], Average Loss: {epoch_loss:.4f}, Training Accuracy: {epoch_acc:.2f}%, Time: {epoch_time:.2f}s')\n",
        "\n",
        "    # Evaluation on the test set after each epoch\n",
        "    model.eval() # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        correct_test = 0\n",
        "        total_test = 0\n",
        "        test_loss = 0.0\n",
        "        for images, audios, labels in test_loader:\n",
        "            images = images.to(device)\n",
        "            audios = audios.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Forward pass: Single output\n",
        "            outputs = model(images, audios)\n",
        "            test_loss += criterion(outputs, labels).item() # Use the single criterion\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total_test += labels.size(0)\n",
        "            correct_test += (predicted == labels).sum().item()\n",
        "\n",
        "        test_loss /= len(test_loader)\n",
        "        test_acc = 100 * correct_test / total_test\n",
        "        print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_acc:.2f}%')\n",
        "\n",
        "        # Optional: Step the scheduler based on validation loss\n",
        "        # if 'scheduler' in locals(): # Check if scheduler was initialized\n",
        "        #    scheduler.step(test_loss)\n",
        "\n",
        "print('Finished Training')\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_QT5LVXP__4",
        "outputId": "f2a6cbc8-1b55-47eb-9896-db5ae3a818c3"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset creation completed. Matched pairs: 290, Mismatched pairs sampled: 290\n",
            "Number of rows in the dataset: 580\n",
            "Label distribution:\n",
            "Akhil: 60\n",
            "Alen: 50\n",
            "Amaan: 30\n",
            "Anshul: 30\n",
            "Neha: 60\n",
            "Shirlyn: 60\n",
            "not identified: 290\n",
            "\n",
            "Total dataset size: 580\n",
            "Training dataset size: 464\n",
            "Test dataset size: 116\n",
            "Using device: cuda\n",
            "\n",
            "Train DataLoader created with 15 batches.\n",
            "Test DataLoader created with 4 batches.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "AttentionFusionModel(\n",
            "  (image_model): ImageCNN_Complex(\n",
            "    (backbone): ResNet(\n",
            "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
            "      (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "      (relu): ReLU(inplace=True)\n",
            "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
            "      (layer1): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "            (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer2): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer3): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (3): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (4): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (5): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (layer4): Sequential(\n",
            "        (0): Bottleneck(\n",
            "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "          (downsample): Sequential(\n",
            "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
            "            (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          )\n",
            "        )\n",
            "        (1): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Bottleneck(\n",
            "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
            "          (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
            "          (relu): ReLU(inplace=True)\n",
            "        )\n",
            "      )\n",
            "      (avgpool): AdaptiveAvgPool2d(output_size=(1, 1))\n",
            "      (fc): Linear(in_features=2048, out_features=512, bias=True)\n",
            "    )\n",
            "  )\n",
            "  (audio_model): AudioRNN_Complex(\n",
            "    (gru): GRU(40, 256, num_layers=2, batch_first=True, bidirectional=True)\n",
            "    (fc): Linear(in_features=512, out_features=512, bias=True)\n",
            "  )\n",
            "  (attention_layer1): Linear(in_features=1024, out_features=128, bias=True)\n",
            "  (attention_layer2): Linear(in_features=128, out_features=2, bias=True)\n",
            "  (fusion_fc): Linear(in_features=1024, out_features=7, bias=True)\n",
            ")\n",
            "\n",
            "Calculating class weights for CrossEntropyLoss...\n",
            "Raw class counts in training data split: defaultdict(<class 'int'>, {1: 39, 6: 223, 4: 49, 2: 25, 5: 52, 0: 51, 3: 25})\n",
            "Calculated class weights: tensor([1.2997, 2.2095, 2.6514, 2.6514, 1.3528, 1.6571, 1.1801])\n",
            "\n",
            "Starting training...\n",
            "Epoch [1/10], Step [10/15], Loss: 1.4132\n",
            "Epoch [1/10], Average Loss: 1.5315, Training Accuracy: 42.03%, Time: 18.55s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 1.2335, Test Accuracy: 47.41%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [2/10], Step [10/15], Loss: 0.9959\n",
            "Epoch [2/10], Average Loss: 1.0217, Training Accuracy: 44.61%, Time: 18.45s\n",
            "Test Loss: 0.8444, Test Accuracy: 56.90%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [3/10], Step [10/15], Loss: 0.8442\n",
            "Epoch [3/10], Average Loss: 0.8613, Training Accuracy: 50.43%, Time: 18.48s\n",
            "Test Loss: 1.0576, Test Accuracy: 41.38%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [4/10], Step [10/15], Loss: 0.6427\n",
            "Epoch [4/10], Average Loss: 0.7876, Training Accuracy: 51.94%, Time: 18.44s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.9355, Test Accuracy: 56.03%\n",
            "Epoch [5/10], Step [10/15], Loss: 1.0195\n",
            "Epoch [5/10], Average Loss: 0.9249, Training Accuracy: 50.65%, Time: 18.41s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.8503, Test Accuracy: 54.31%\n",
            "Epoch [6/10], Step [10/15], Loss: 0.9642\n",
            "Epoch [6/10], Average Loss: 0.9182, Training Accuracy: 45.69%, Time: 18.13s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7586, Test Accuracy: 49.14%\n",
            "Epoch [7/10], Step [10/15], Loss: 0.8356\n",
            "Epoch [7/10], Average Loss: 0.7820, Training Accuracy: 50.22%, Time: 18.51s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.8004, Test Accuracy: 43.97%\n",
            "Epoch [8/10], Step [10/15], Loss: 0.8092\n",
            "Epoch [8/10], Average Loss: 0.7192, Training Accuracy: 52.37%, Time: 19.00s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.7118, Test Accuracy: 53.45%\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [9/10], Step [10/15], Loss: 0.5814\n",
            "Epoch [9/10], Average Loss: 0.6914, Training Accuracy: 53.45%, Time: 19.70s\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/utils/data/dataloader.py:624: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test Loss: 0.8612, Test Accuracy: 44.83%\n",
            "Epoch [10/10], Step [10/15], Loss: 0.7197\n",
            "Epoch [10/10], Average Loss: 0.6537, Training Accuracy: 62.72%, Time: 19.92s\n",
            "Test Loss: 0.6453, Test Accuracy: 59.48%\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "model.eval() # Set model to evaluation mode\n",
        "all_labels = []\n",
        "all_predictions = []\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, audios, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        audios = audios.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images, audios)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        all_labels.extend(labels.cpu().numpy())\n",
        "        all_predictions.extend(predicted.cpu().numpy())\n",
        "\n",
        "# Get class names, including 'not identified'\n",
        "class_names = [train_dataset.dataset.idx_to_person[i] for i in sorted(train_dataset.dataset.idx_to_person.keys())]\n",
        "\n",
        "# Compute confusion matrix\n",
        "cm = confusion_matrix(all_labels, all_predictions)\n",
        "\n",
        "# Display confusion matrix\n",
        "plt.figure(figsize=(10, 8))\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix on Test Set')\n",
        "plt.show()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 489
        },
        "id": "s_0Dh5yCdNE7",
        "outputId": "e2ca1baf-5b3b-4672-d363-bc0947372acc"
      },
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 1000x800 with 0 Axes>"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 2 Axes>"
            ],
            "image/png": "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\n"
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model.eval() # Set model to evaluation mode\n",
        "\n",
        "matched_correct = 0\n",
        "matched_total = 0\n",
        "mismatched_correct = 0\n",
        "mismatched_total = 0\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, audios, labels in test_loader:\n",
        "        images = images.to(device)\n",
        "        audios = audios.to(device)\n",
        "        labels = labels.to(device)\n",
        "\n",
        "        outputs = model(images, audios)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "\n",
        "        for i in range(labels.size(0)):\n",
        "            # original_index_in_dataset = test_data.indices[i + test_loader.batch_size * (test_loader.batch_sampler.batch_iter.__self__.batch_indices_history[-1] // test_loader.batch_size)] if hasattr(test_loader.batch_sampler, 'batch_indices_history') else -1\n",
        "            # Note: Getting the exact original index from the Subset can be complex in iterating batches.\n",
        "            # For simplicity here, we rely on the label itself to identify matched/mismatched.\n",
        "            # A more robust way might involve modifying the Dataset or DataLoader to return pair info.\n",
        "\n",
        "            # Access idx_to_person from the original dataset object via .dataset\n",
        "            label_name = test_dataset.dataset.idx_to_person[labels[i].item()]\n",
        "            predicted_name = test_dataset.dataset.idx_to_person[predicted[i].item()]\n",
        "\n",
        "            if label_name != 'not identified': # This is a matched pair\n",
        "                matched_total += 1\n",
        "                if predicted[i].item() == labels[i].item():\n",
        "                    matched_correct += 1\n",
        "            else: # This is a mismatched pair\n",
        "                mismatched_total += 1\n",
        "                if predicted[i].item() == labels[i].item():\n",
        "                    mismatched_correct += 1\n",
        "\n",
        "matched_accuracy = (matched_correct / matched_total) * 100 if matched_total > 0 else 0\n",
        "mismatched_accuracy = (mismatched_correct / mismatched_total) * 100 if mismatched_total > 0 else 0\n",
        "\n",
        "print(\"\\n--- Matched vs Mismatched Pair Evaluation ---\")\n",
        "print(f\"Total Matched Pairs in Test Set: {matched_total}\")\n",
        "print(f\"Correctly Classified Matched Pairs: {matched_correct}\")\n",
        "print(f\"Accuracy on Matched Pairs: {matched_accuracy:.2f}%\")\n",
        "\n",
        "print(f\"\\nTotal Mismatched Pairs in Test Set: {mismatched_total}\")\n",
        "print(f\"Correctly Classified Mismatched Pairs: {mismatched_correct}\")\n",
        "print(f\"Accuracy on Mismatched Pairs: {mismatched_accuracy:.2f}%\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-GxUQ5-BWrHN",
        "outputId": "11b213d6-b99f-4875-ef6e-19d42178c561"
      },
      "execution_count": 59,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "--- Matched vs Mismatched Pair Evaluation ---\n",
            "Total Matched Pairs in Test Set: 49\n",
            "Correctly Classified Matched Pairs: 45\n",
            "Accuracy on Matched Pairs: 91.84%\n",
            "\n",
            "Total Mismatched Pairs in Test Set: 67\n",
            "Correctly Classified Mismatched Pairs: 25\n",
            "Accuracy on Mismatched Pairs: 37.31%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model_save_path = 'attention_fusion_model.pth'\n",
        "\n",
        "# Save the model state dictionary\n",
        "torch.save(model.state_dict(), model_save_path)\n",
        "\n",
        "print(f\"Model saved successfully to {model_save_path}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FG07FlRX3nPm",
        "outputId": "a3db4887-7b41-4197-95cb-adb7a32a8020"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model saved successfully to attention_fusion_model.pth\n"
          ]
        }
      ]
    }
  ]
}